import tensorflow as tf   # using for trainning deep learning model
from tensorflow.keras.models import Sequential   #linear stack of layer 
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout  #conv2d extract feature from image , maxpooling reduce the spatial size , flatten convert into 1d 
from tensorflow.keras.preprocessing.image import ImageDataGenerator  #automatically resize , normalize the image 
import numpy as np
import matplotlib.pyplot as plt
import cv2
import os



#this FER 2013 dataset contain 48x48 grayscale image with 7 diffrent label 
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_path = "dataset/train"  
test_path = "dataset/test"    

datagen = ImageDataGenerator(rescale=1./255)

train_generator = datagen.flow_from_directory(
    train_path,
    target_size=(48, 48),  #resize all images to 48x48 pixels
    batch_size=64,   #load 64 images at a time for trainning
    color_mode="grayscale",
    class_mode="categorical"  
)

val_generator = datagen.flow_from_directory(
    test_path,  
    target_size=(48, 48),
    batch_size=64,
    color_mode="grayscale",
    class_mode="categorical"
)






model = Sequential([
    Conv2D(64, (3,3), activation='relu', input_shape=(48, 48, 1)),  #relu remove negative pixel value
    MaxPooling2D(2,2), # reduce the image size by half from 48x48 to 24x24
    #reduce computational cost 
    
    Conv2D(128, (3,3), activation='relu'),  #128 filter extract deeper features
    MaxPooling2D(2,2), #again reduce the image size from 24x24 to 12x12
    #extract more pattern like eye shape , nose.
    
    Conv2D(256, (3,3), activation='relu'),  #256 filter extract even deeper
    MaxPooling2D(2,2), #again reduce the image size from 12x12 to 6x6 
    #detect high level facial feature like emotion 

    Flatten(),  ##convert 2D features maps into 1D vector
    Dense(512, activation='relu'),  #a fully connectd layer with 512 neurons  
    Dropout(0.5),  #50% neurons are randomly disabled during training to improve generalization
    Dense(7, activation='softmax')    #7 neurons for each emotion and convert value into prbabaility sum =1 for each emotion 
])


model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])


model.summary()




history = model.fit(train_generator, validation_data=val_generator, epochs=25)
#model go through 25 times to improve accuracy 

model.save("facial_expression_model1.h5")
#save the trained model 


# Evaluate on validation set
val_loss, val_acc = model.evaluate(val_generator)
print(f"Validation Accuracy: {val_acc:.2f}")



# Load model and labels
model = tf.keras.models.load_model("facial_expression_model1.h5")
emotion_labels = ["Angry", "Disgust", "Fear", "Happy", "Neutral", "Sad", "Surprise"]

# Start webcam
cap = cv2.VideoCapture(0)
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
#pre trainned model that detect human face

while True:
    ret, frame = cap.read()  #if true frame is successfully captured
    if not ret:
        break

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)  #reduce 30% per iteration 

    for (x, y, w, h) in faces:
        roi_gray = gray[y:y+h, x:x+w]
        roi_gray = cv2.resize(roi_gray, (48, 48))
        roi_gray = np.expand_dims(roi_gray, axis=0)
        roi_gray = np.expand_dims(roi_gray, axis=-1)
        roi_gray = roi_gray / 255.0  # Normalize pixel value between 0 and 1 

        prediction = model.predict(roi_gray)
        emotion = emotion_labels[np.argmax(prediction)]

        # Display the emotion label on the frame 
        cv2.putText(frame, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)

    cv2.imshow('Facial Expression Recognition', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()




